{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use the Resnet that we trained to annotate images.\n",
    "The image annotations consist of bounding boxes which envelope a single object and labels for each object in the bounding box. Our training set is given by COCO 2014.\n",
    "\n",
    "Our first task is to modify the the neural network that we trained in file 1 to be adapted for the much smaller list of classes in COCO. COCO consists of 80 categories:\n",
    "\n",
    "person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic_light, fire_hydrant, stop_sign, parking_meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports_ball, kite, baseball_bat, baseball_glove, skateboard, surfboard, tennis_racket, bottle, wine_glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot_dog, pizza, donut, cake, chair, couch, potted_plant, bed, dining_table, toilet, tv, laptop, mouse, remote, keyboard, cell_phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy_bear, hair_drier, toothbrush\n",
    "\n",
    "In order to facilitate the smaller class, set we we freeze the hidden layers of the network and learn the fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resnet18(num_classes=1000):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: /home/ryan/Deep_Learning/resnet18_weights.pth\n",
      "Model loaded, but fc layer was ignored due to size mismatch.\n",
      "Model loaded and transferred to: cpu\n",
      "Frozen: conv1.weight\n",
      "Frozen: bn1.weight\n",
      "Frozen: bn1.bias\n",
      "Frozen: layer1.0.conv1.weight\n",
      "Frozen: layer1.0.bn1.weight\n",
      "Frozen: layer1.0.bn1.bias\n",
      "Frozen: layer1.0.conv2.weight\n",
      "Frozen: layer1.0.bn2.weight\n",
      "Frozen: layer1.0.bn2.bias\n",
      "Frozen: layer1.1.conv1.weight\n",
      "Frozen: layer1.1.bn1.weight\n",
      "Frozen: layer1.1.bn1.bias\n",
      "Frozen: layer1.1.conv2.weight\n",
      "Frozen: layer1.1.bn2.weight\n",
      "Frozen: layer1.1.bn2.bias\n",
      "Frozen: layer2.0.conv1.weight\n",
      "Frozen: layer2.0.bn1.weight\n",
      "Frozen: layer2.0.bn1.bias\n",
      "Frozen: layer2.0.conv2.weight\n",
      "Frozen: layer2.0.bn2.weight\n",
      "Frozen: layer2.0.bn2.bias\n",
      "Frozen: layer2.0.downsample.0.weight\n",
      "Frozen: layer2.0.downsample.1.weight\n",
      "Frozen: layer2.0.downsample.1.bias\n",
      "Frozen: layer2.1.conv1.weight\n",
      "Frozen: layer2.1.bn1.weight\n",
      "Frozen: layer2.1.bn1.bias\n",
      "Frozen: layer2.1.conv2.weight\n",
      "Frozen: layer2.1.bn2.weight\n",
      "Frozen: layer2.1.bn2.bias\n",
      "Frozen: layer3.0.conv1.weight\n",
      "Frozen: layer3.0.bn1.weight\n",
      "Frozen: layer3.0.bn1.bias\n",
      "Frozen: layer3.0.conv2.weight\n",
      "Frozen: layer3.0.bn2.weight\n",
      "Frozen: layer3.0.bn2.bias\n",
      "Frozen: layer3.0.downsample.0.weight\n",
      "Frozen: layer3.0.downsample.1.weight\n",
      "Frozen: layer3.0.downsample.1.bias\n",
      "Frozen: layer3.1.conv1.weight\n",
      "Frozen: layer3.1.bn1.weight\n",
      "Frozen: layer3.1.bn1.bias\n",
      "Frozen: layer3.1.conv2.weight\n",
      "Frozen: layer3.1.bn2.weight\n",
      "Frozen: layer3.1.bn2.bias\n",
      "Training: layer4.0.conv1.weight\n",
      "Training: layer4.0.bn1.weight\n",
      "Training: layer4.0.bn1.bias\n",
      "Training: layer4.0.conv2.weight\n",
      "Training: layer4.0.bn2.weight\n",
      "Training: layer4.0.bn2.bias\n",
      "Training: layer4.0.downsample.0.weight\n",
      "Training: layer4.0.downsample.1.weight\n",
      "Training: layer4.0.downsample.1.bias\n",
      "Training: layer4.1.conv1.weight\n",
      "Training: layer4.1.bn1.weight\n",
      "Training: layer4.1.bn1.bias\n",
      "Training: layer4.1.conv2.weight\n",
      "Training: layer4.1.bn2.weight\n",
      "Training: layer4.1.bn2.bias\n",
      "Training: fc.weight\n",
      "Training: fc.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5307/4009673036.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Get Git repository root dynamically\n",
    "repo_root = subprocess.check_output(['git', 'rev-parse', '--show-toplevel'], text=True).strip()\n",
    "\n",
    "# Define path relative to Git repo root\n",
    "weights_path = Path(repo_root) / \"resnet18_weights.pth\"\n",
    "\n",
    "#weights_path = '/home/ryan/Deep_Learning/resnet18_weights.pth'\n",
    "print(f\"Loading weights from: {weights_path}\")\n",
    "\n",
    "num_coco_classes = 80  # Adjust for your dataset\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_coco_classes)\n",
    "\n",
    "# Load the checkpoint, but ignore incompatible layers\n",
    "checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "\n",
    "# Remove the fc layer weights from the loaded checkpoint\n",
    "del checkpoint['fc.weight']\n",
    "del checkpoint['fc.bias']\n",
    "\n",
    "# Load the model ignoring the fc layer\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "print(\"Model loaded, but fc layer was ignored due to size mismatch.\")\n",
    "\n",
    "# Define new fully connected layer with correct number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_coco_classes)\n",
    "\n",
    "# Move model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model loaded and transferred to:\", device)\n",
    "\n",
    "\n",
    "# Freeze all layers except the last block and fc\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer4' not in name and 'fc' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Training: {name}\")\n",
    "    else:\n",
    "        print(f\"Frozen: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=6.69s)\n",
      "creating index...\n",
      "index created!\n",
      "seen all proposals\n",
      "tensor(0.0051)\n",
      "tensor(0.2049)\n",
      "tensor(0.0264)\n",
      "tensor(0.0488)\n",
      "tensor(0.0652)\n",
      "tensor(0.0642)\n",
      "tensor(0.1515)\n",
      "tensor(0.0470)\n",
      "tensor(0.1032)\n",
      "tensor(0.1157)\n",
      "tensor(0.0448)\n",
      "tensor(0.0331)\n",
      "tensor(0.1548)\n",
      "tensor(0.)\n",
      "tensor(0.1023)\n",
      "tensor(0.0101)\n",
      "tensor(0.0414)\n",
      "tensor(0.2362)\n",
      "tensor(0.0153)\n",
      "tensor(0.0064)\n",
      "tensor(0.0211)\n",
      "tensor(0.0073)\n",
      "tensor(0.0459)\n",
      "tensor(0.5779)\n",
      "tensor([6])\n",
      "tensor([28])\n",
      "tensor([[-0.0218, -0.1217, -0.1882, -0.1074, -0.2919,  0.0997,  0.1103,  0.0212,\n",
      "         -0.1436,  0.1472,  0.0996, -0.0359, -0.0186, -0.1882,  0.0265, -0.2571,\n",
      "          0.0514,  0.1464, -0.0256, -0.1701, -0.1303,  0.1301,  0.1774,  0.0331,\n",
      "          0.0193, -0.1946,  0.2867, -0.0316,  0.3479,  0.1909,  0.2999,  0.1118,\n",
      "         -0.0158, -0.1441, -0.1471, -0.2155,  0.0043,  0.0791,  0.3419, -0.1553,\n",
      "         -0.0833,  0.0590,  0.3118, -0.0643,  0.0057,  0.2070,  0.2763,  0.2574,\n",
      "          0.1620,  0.0859,  0.0170, -0.1926,  0.2296,  0.0536,  0.2401,  0.0567,\n",
      "         -0.0470,  0.2636, -0.2115, -0.2049, -0.2157, -0.1823, -0.0988,  0.0548,\n",
      "         -0.1811, -0.1491, -0.3418, -0.1336, -0.2207,  0.1120,  0.1785, -0.2295,\n",
      "         -0.1158, -0.0612, -0.3434,  0.2165, -0.3376,  0.1412, -0.2672, -0.2208]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([6])\n",
      "tensor(0.)\n",
      "tensor(0.0094)\n",
      "tensor(0.1343)\n",
      "tensor(0.0046)\n",
      "tensor(0.1113)\n",
      "tensor(0.0634)\n",
      "tensor(0.0696)\n",
      "tensor(0.)\n",
      "tensor(0.5524)\n",
      "tensor([8])\n",
      "tensor([28])\n",
      "tensor([[-0.0483, -0.1687, -0.1728, -0.1345, -0.2993,  0.1267,  0.1080,  0.0973,\n",
      "         -0.1870,  0.1299,  0.0881, -0.0732,  0.0111, -0.2192,  0.0355, -0.2327,\n",
      "          0.0565,  0.1946, -0.0467, -0.1666, -0.1385,  0.1152,  0.2325,  0.0851,\n",
      "          0.0203, -0.2247,  0.3162, -0.0207,  0.3436,  0.1815,  0.2789,  0.1045,\n",
      "         -0.0456, -0.1720, -0.1080, -0.1710,  0.0583,  0.0553,  0.3378, -0.1962,\n",
      "         -0.1308,  0.0138,  0.3012, -0.0615,  0.0245,  0.2242,  0.2525,  0.2724,\n",
      "          0.1741,  0.0308,  0.0471, -0.1617,  0.2621,  0.0535,  0.2972,  0.0532,\n",
      "         -0.0729,  0.2545, -0.2057, -0.2177, -0.2335, -0.1214, -0.0849,  0.0691,\n",
      "         -0.1851, -0.1809, -0.2882, -0.1334, -0.2606,  0.1335,  0.2236, -0.2226,\n",
      "         -0.1048, -0.1082, -0.3442,  0.2160, -0.3580,  0.1479, -0.2645, -0.2246]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([8])\n",
      "tensor(0.0051)\n",
      "tensor(0.0389)\n",
      "tensor(0.0544)\n",
      "tensor(0.0074)\n",
      "tensor(0.0259)\n",
      "tensor(0.0378)\n",
      "tensor(0.4411)\n",
      "tensor(0.0478)\n",
      "tensor(0.0096)\n",
      "tensor(0.0165)\n",
      "tensor(0.6045)\n",
      "tensor([6])\n",
      "tensor([38])\n",
      "tensor([[-0.0221, -0.1060, -0.1582, -0.1887, -0.2717,  0.0865,  0.0652,  0.0450,\n",
      "         -0.1205,  0.1475,  0.0793, -0.1322, -0.0053, -0.2298,  0.0563, -0.2459,\n",
      "          0.0102,  0.1623, -0.1095, -0.1914, -0.1296,  0.1486,  0.1541,  0.0767,\n",
      "          0.0412, -0.2284,  0.2854,  0.0150,  0.3175,  0.1728,  0.3106,  0.1399,\n",
      "         -0.0395, -0.2089, -0.1325, -0.1683,  0.0616,  0.0646,  0.3756, -0.1709,\n",
      "         -0.0174,  0.1017,  0.2962, -0.0523,  0.0145,  0.1812,  0.2238,  0.2023,\n",
      "          0.1662,  0.0865,  0.0089, -0.1627,  0.2207,  0.0863,  0.2834,  0.0349,\n",
      "         -0.0162,  0.2876, -0.2341, -0.2118, -0.2076, -0.1411, -0.0787,  0.1265,\n",
      "         -0.1445, -0.1559, -0.3176, -0.1209, -0.2759,  0.1573,  0.2512, -0.1854,\n",
      "         -0.1365, -0.0685, -0.3254,  0.2373, -0.3472,  0.1365, -0.2700, -0.2296]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([6])\n",
      "tensor(0.0244)\n",
      "tensor(0.0536)\n",
      "tensor(0.0475)\n",
      "tensor(0.0434)\n",
      "tensor(0.0819)\n",
      "tensor(0.5170)\n",
      "tensor([6])\n",
      "tensor([38])\n",
      "tensor([[ 0.0303, -0.0368, -0.1341, -0.0050, -0.2026,  0.1444,  0.0598,  0.0432,\n",
      "         -0.1256,  0.0787,  0.1131, -0.0491, -0.0517, -0.1820, -0.0396, -0.1522,\n",
      "          0.0266,  0.1282, -0.0239, -0.1059, -0.0534,  0.1046,  0.1006,  0.0760,\n",
      "          0.0014, -0.1805,  0.2424, -0.0640,  0.2556,  0.2102,  0.1882,  0.0957,\n",
      "         -0.0025, -0.0483, -0.1094, -0.1693,  0.0073,  0.0784,  0.3066, -0.1417,\n",
      "         -0.0586,  0.0367,  0.1971, -0.0825,  0.0555,  0.1919,  0.2218,  0.2095,\n",
      "          0.0973,  0.1241,  0.0131, -0.1441,  0.1909,  0.1254,  0.2263, -0.0023,\n",
      "          0.0374,  0.1910, -0.1606, -0.1319, -0.1086, -0.1601, -0.0712,  0.0174,\n",
      "         -0.1733, -0.1578, -0.2515, -0.0601, -0.1879,  0.0889,  0.1565, -0.1609,\n",
      "         -0.1524, -0.0324, -0.2210,  0.2067, -0.2387,  0.1170, -0.2076, -0.2041]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([6])\n",
      "tensor(0.0279)\n",
      "tensor(0.2910)\n",
      "tensor(0.0038)\n",
      "tensor(0.4961)\n",
      "tensor(0.)\n",
      "tensor(0.0519)\n",
      "tensor(0.1696)\n",
      "tensor(0.0155)\n",
      "tensor(0.0129)\n",
      "tensor(0.0066)\n",
      "tensor(0.0196)\n",
      "tensor(0.3939)\n",
      "tensor(0.0155)\n",
      "tensor(0.0919)\n",
      "tensor(0.0126)\n",
      "tensor(0.0069)\n",
      "tensor(0.1277)\n",
      "tensor(0.0551)\n",
      "tensor(0.5770)\n",
      "tensor([6])\n",
      "tensor([38])\n",
      "tensor([[-0.0368, -0.1143, -0.1428, -0.2132, -0.2672,  0.1105,  0.0732,  0.0412,\n",
      "         -0.1303,  0.1661,  0.0898, -0.1207,  0.0054, -0.2388,  0.0659, -0.2802,\n",
      "          0.0169,  0.1726, -0.0936, -0.2064, -0.1314,  0.1653,  0.1421,  0.0652,\n",
      "          0.0214, -0.1986,  0.3194, -0.0080,  0.3251,  0.1804,  0.3051,  0.1498,\n",
      "         -0.0442, -0.2122, -0.1063, -0.1743,  0.0496,  0.0883,  0.3720, -0.1649,\n",
      "         -0.0152,  0.0919,  0.3014, -0.0515,  0.0434,  0.1805,  0.2176,  0.2024,\n",
      "          0.1563,  0.0768, -0.0229, -0.1492,  0.2554,  0.0731,  0.3144,  0.0554,\n",
      "         -0.0418,  0.2486, -0.2507, -0.2328, -0.2336, -0.1812, -0.1102,  0.1188,\n",
      "         -0.1374, -0.1468, -0.3074, -0.0967, -0.2539,  0.1133,  0.2555, -0.2103,\n",
      "         -0.1048, -0.0805, -0.3713,  0.2404, -0.3549,  0.1372, -0.2927, -0.2268]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([6])\n",
      "tensor(0.2772)\n",
      "tensor(0.0305)\n",
      "tensor(0.0057)\n",
      "tensor(0.)\n",
      "tensor(0.0300)\n",
      "tensor(0.0231)\n",
      "tensor(0.0216)\n",
      "tensor(0.0449)\n",
      "tensor(0.0426)\n",
      "tensor(0.0038)\n",
      "tensor(0.0335)\n",
      "tensor(0.)\n",
      "tensor(0.0657)\n",
      "tensor(0.4506)\n",
      "tensor(0.0658)\n",
      "tensor(0.)\n",
      "tensor(0.0051)\n",
      "tensor(0.0286)\n",
      "tensor(0.0243)\n",
      "tensor(0.0132)\n",
      "tensor(0.0052)\n",
      "tensor(0.7045)\n",
      "tensor([6])\n",
      "tensor([38])\n",
      "tensor([[-3.0538e-02, -9.8927e-02, -1.4444e-01, -1.2429e-01, -2.2975e-01,\n",
      "          1.0619e-01,  6.8627e-02,  5.7896e-02, -1.4108e-01,  9.9321e-02,\n",
      "          8.1667e-02, -1.0275e-01,  2.6059e-03, -2.3075e-01,  1.1985e-02,\n",
      "         -2.4945e-01,  2.9832e-03,  1.4348e-01, -3.7133e-02, -1.3542e-01,\n",
      "         -8.9183e-02,  1.2534e-01,  1.3747e-01,  1.0126e-01,  1.3048e-02,\n",
      "         -1.7124e-01,  2.9717e-01, -9.8294e-05,  3.2595e-01,  2.1273e-01,\n",
      "          2.5424e-01,  1.5047e-01, -2.1005e-02, -1.6379e-01, -1.4005e-01,\n",
      "         -1.7999e-01,  4.0734e-02,  8.3751e-02,  3.4950e-01, -1.6354e-01,\n",
      "         -2.1614e-02,  6.8943e-02,  2.6498e-01, -2.5058e-02,  3.8148e-02,\n",
      "          1.4538e-01,  2.3336e-01,  1.9684e-01,  1.5571e-01,  8.6040e-02,\n",
      "          1.8988e-02, -1.6172e-01,  2.1186e-01,  7.0975e-02,  2.5642e-01,\n",
      "          4.1059e-02, -1.3903e-02,  2.3820e-01, -2.3320e-01, -1.9532e-01,\n",
      "         -2.3032e-01, -1.8312e-01, -4.9519e-02,  4.4075e-02, -1.4012e-01,\n",
      "         -1.9628e-01, -2.9608e-01, -1.1281e-01, -2.4429e-01,  1.4027e-01,\n",
      "          2.3622e-01, -1.9026e-01, -9.1797e-02, -3.7173e-02, -3.1583e-01,\n",
      "          2.5559e-01, -3.2673e-01,  1.2803e-01, -2.2460e-01, -2.1954e-01]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([6])\n",
      "tensor(0.0523)\n",
      "tensor(0.0174)\n",
      "tensor(0.0026)\n",
      "tensor(0.0107)\n",
      "tensor(0.0417)\n",
      "tensor(0.0618)\n",
      "tensor(0.5782)\n",
      "tensor([6])\n",
      "tensor([38])\n",
      "tensor([[-0.0315, -0.1139, -0.1400, -0.1917, -0.2725,  0.1133,  0.0690,  0.0305,\n",
      "         -0.1237,  0.1640,  0.0671, -0.1196, -0.0099, -0.2325,  0.0610, -0.2778,\n",
      "          0.0241,  0.1597, -0.0924, -0.2018, -0.1276,  0.1584,  0.1293,  0.0646,\n",
      "          0.0247, -0.1984,  0.3154,  0.0005,  0.3234,  0.1878,  0.2949,  0.1647,\n",
      "         -0.0394, -0.2059, -0.1013, -0.1703,  0.0475,  0.0811,  0.3706, -0.1577,\n",
      "         -0.0128,  0.1023,  0.2985, -0.0601,  0.0316,  0.1726,  0.2193,  0.2111,\n",
      "          0.1712,  0.0790, -0.0202, -0.1545,  0.2559,  0.0791,  0.2969,  0.0553,\n",
      "         -0.0383,  0.2547, -0.2520, -0.2128, -0.2384, -0.1730, -0.1131,  0.0983,\n",
      "         -0.1425, -0.1540, -0.3039, -0.0942, -0.2477,  0.1090,  0.2569, -0.2186,\n",
      "         -0.1137, -0.0658, -0.3594,  0.2388, -0.3447,  0.1359, -0.2964, -0.2270]],\n",
      "       grad_fn=<AddmmBackward0>) tensor([6])\n",
      "tensor(0.0108)\n",
      "tensor(0.0076)\n",
      "tensor(0.0286)\n",
      "tensor(0.0276)\n",
      "tensor(0.0109)\n",
      "tensor(0.0004)\n",
      "tensor(0.0540)\n",
      "tensor(0.0168)\n",
      "tensor(0.0093)\n",
      "tensor(0.0861)\n",
      "tensor(0.0473)\n",
      "tensor(0.0191)\n",
      "tensor(0.0074)\n",
      "tensor(0.2903)\n",
      "tensor(0.0393)\n",
      "tensor(0.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 174\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 155\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, confidence_threshold, save_dir)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accumulated_loss \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m         accumulated_loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagate accumulated loss\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update model after processing all proposals\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Perform validation\u001b[39;00m\n\u001b[1;32m    158\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate_model()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:197\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:430\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_compiling()\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built()\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    429\u001b[0m     ):\n\u001b[0;32m--> 430\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    433\u001b[0m             group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[1;32m    434\u001b[0m         ):\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    436\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    438\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m             )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/graphs.py:30\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CocoDetection\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.ops import box_iou\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Path to the COCO dataset\n",
    "coco_root = \"/home/ryan/Documents/COCO/train2014\"  # Update with your dataset path\n",
    "coco_ann = \"/home/ryan/Documents/COCO/annotations/instances_train2014.json\"  # Update annotation path\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the COCO dataset\n",
    "full_dataset = CocoDetection(root=coco_root, annFile=coco_ann, transform=transform)\n",
    "\n",
    "# Split dataset into 80% training and 20% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Function to generate proposals using Selective Search\n",
    "def get_selective_search_proposals(image_path):\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    image = cv2.imread(image_path)\n",
    "    ss.setBaseImage(image)\n",
    "    ss.switchToSelectiveSearchQuality()  # Higher quality proposals\n",
    "    rects = ss.process()\n",
    "    return rects[:200]  # Limit number of proposals\n",
    "\n",
    "# Function to compute IoU\n",
    "def compute_iou(true_boxes, pred_boxes):\n",
    "    true_boxes = torch.tensor(true_boxes, dtype=torch.float32)\n",
    "    pred_boxes = torch.tensor(pred_boxes, dtype=torch.float32)\n",
    "    return box_iou(true_boxes, pred_boxes)\n",
    "\n",
    "# Function to validate the model\n",
    "def validate_model():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img, target in val_loader:\n",
    "            img = img.to(device)\n",
    "            gt_labels = [ann['category_id'] for ann in target]\n",
    "            outputs = model(img)\n",
    "            labels = torch.tensor(gt_labels, dtype=torch.long).to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# Convert COCO bbox format [x, y, w, h] -> [x_min, y_min, x_max, y_max]\n",
    "def coco_to_xyxy(box):\n",
    "    x_min, y_min, width, height = box\n",
    "    x_max = x_min + width\n",
    "    y_max = y_min + height\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "\n",
    "def train_model(num_epochs=10, confidence_threshold=0.0, save_dir=\"/home/ryan/Documents/Deep_Learning\"):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for img, target in train_loader:  # Process images one at a time\n",
    "            img = img.to(device)  # Move image to GPU\n",
    "            img_id = target[0]['image_id'].item()  # Convert tensor to integer\n",
    "            img_path = full_dataset.coco.loadImgs(img_id)[0]['file_name']\n",
    "            img_full_path = f\"{coco_root}/{img_path}\"\n",
    "\n",
    "            # Generate bounding box proposals using Selective Search\n",
    "            proposals = get_selective_search_proposals(img_full_path)\n",
    "\n",
    "            # Get ground truth boxes and labels\n",
    "            gt_boxes = [ann['bbox'] for ann in target]\n",
    "            gt_labels = [ann['category_id'] for ann in target]\n",
    "\n",
    "            accumulated_loss = 0.0  # Accumulate loss for proposals\n",
    "\n",
    "            optimizer.zero_grad()  # Zero gradients before processing proposals\n",
    "\n",
    "            min_size = 20  # Minimum acceptable height/width\n",
    "\n",
    "            accumulated_loss = torch.tensor(0.0, device=device)  # Initialize as a tensor\n",
    "\n",
    "            print('seen all proposals')\n",
    "            for (x, y, w, h) in proposals:\n",
    "                # Ensure bounding box coordinates are valid and not too small\n",
    "                if w <= min_size or h <= min_size or x < 0 or y < 0 or x + w > img.shape[3] or y + h > img.shape[2]:\n",
    "                    continue\n",
    "\n",
    "                # Crop and resize the region of interest (ROI)\n",
    "                roi = img[:, :, y:y + h, x:x + w]\n",
    "                roi_resized = torch.nn.functional.interpolate(roi, size=(224, 224), mode='bilinear', align_corners=False).to(device)\n",
    "\n",
    "                gt_boxes_xyxy = [coco_to_xyxy(bbox) for bbox in gt_boxes]\n",
    "                pred_box_xyxy = coco_to_xyxy([x, y, w, h])\n",
    "\n",
    "                iou = compute_iou(gt_boxes_xyxy, [pred_box_xyxy])\n",
    "\n",
    "                # Assign correct label based on IoU\n",
    "                print(iou.max())\n",
    "                if iou.max() > 0.5:\n",
    "                    label = gt_labels[iou.argmax().item()]\n",
    "                    print(label)\n",
    "                else:\n",
    "                    continue \n",
    "\n",
    "                labels = torch.tensor([label], dtype=torch.long).to(device)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                outputs = model(roi_resized)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                confidence, pred_label = torch.max(probs, 1)\n",
    "                print(pred_label)\n",
    "                # Skip low-confidence proposals\n",
    "                if confidence.item() < confidence_threshold:\n",
    "                    continue\n",
    "                loss = criterion(outputs, labels)\n",
    "                print(outputs, labels)\n",
    "                # Accumulate loss across all high-confidence proposals\n",
    "                accumulated_loss += loss\n",
    "\n",
    "            if accumulated_loss > 0:\n",
    "                accumulated_loss.backward()  # Backpropagate accumulated loss\n",
    "                optimizer.step()  # Update model after processing all proposals\n",
    "\n",
    "        # Perform validation\n",
    "        val_loss = validate_model()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_path = os.path.join(save_dir, f'resnet50_best.pth')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"New best model saved to {save_path}\")\n",
    "        else:\n",
    "            print(\"Early stopping, validation loss did not improve.\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Train the model\n",
    "train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
